{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.17,>=2.16 (from tf-keras)\n",
      "  Downloading tensorflow-2.16.2-cp312-cp312-macosx_10_15_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading grpcio-1.68.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading optree-0.13.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.16.2-cp312-cp312-macosx_10_15_x86_64.whl (259.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.0-cp312-cp312-macosx_10_9_universal2.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl (393 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-macosx_10_13_universal2.whl (600 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.0/601.0 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow, tf-keras\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.0 keras-3.6.0 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 termcolor-2.5.0 tf-keras-2.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4s/ccclfjqs2v504c8lk55hwgj40000gn/T/ipykernel_79475/2896917563.py:10: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  aruba_data = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Value</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Begin_End</th>\n",
       "      <th>Date_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M003</td>\n",
       "      <td>ON</td>\n",
       "      <td>Sleeping</td>\n",
       "      <td>begin</td>\n",
       "      <td>2010-11-04 00:03:50.209589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M003</td>\n",
       "      <td>OFF</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:03:57.399391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T002</td>\n",
       "      <td>21.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:15:08.984841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T003</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:30:19.185547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T004</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:30:19.385336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sensor Value  Activity Begin_End                  Date_Time\n",
       "0   M003    ON  Sleeping     begin 2010-11-04 00:03:50.209589\n",
       "1   M003   OFF                     2010-11-04 00:03:57.399391\n",
       "2   T002  21.5                     2010-11-04 00:15:08.984841\n",
       "3   T003    21                     2010-11-04 00:30:19.185547\n",
       "4   T004    21                     2010-11-04 00:30:19.385336"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMAC\n",
    "# aruba_data_path = '/Users/harrisonkirstein/Documents/GitHub/CSCI-4380-Honors-Option-Repo/CSCI 4380 Honors Option Project/Datasets/aruba/data'\n",
    "\n",
    "\n",
    "# LAPTOP\n",
    "aruba_data_path = '/Users/harrisonkirstein/Desktop/CSCI-4380-Honors-Option-Repo/CSCI 4380 Honors Option Project/Datasets/aruba/data'\n",
    "\n",
    "\n",
    "# Load dataset with variable columns\n",
    "aruba_data = pd.read_csv(\n",
    "    aruba_data_path, \n",
    "    header=None, \n",
    "    names=['Date', 'Time', 'Sensor', 'Value', 'Activity', 'Begin_End'], \n",
    "    delim_whitespace=True,\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "# Combine Date and Time into a single timestamp column\n",
    "aruba_data['Date_Time'] = pd.to_datetime(aruba_data['Date'] + ' ' + aruba_data['Time'], errors='coerce')\n",
    "aruba_data.drop(columns=['Date', 'Time'], inplace=True)\n",
    "\n",
    "# Fill missing columns with NaN for rows without activity labels\n",
    "aruba_data.fillna('', inplace=True)\n",
    "\n",
    "# Preview the dataset\n",
    "aruba_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Value</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Begin_End</th>\n",
       "      <th>Date_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1476693</th>\n",
       "      <td>c</td>\n",
       "      <td>OFF</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2011-05-10 18:42:45.169231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sensor Value Activity Begin_End                  Date_Time\n",
       "1476693      c   OFF                    2011-05-10 18:42:45.169231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aruba_data[aruba_data['Sensor'] == 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create Activity Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of activity windows: 6441\n"
     ]
    }
   ],
   "source": [
    "# Group sensor data into activity windows\n",
    "def create_activity_windows(data):\n",
    "    windows = []\n",
    "    current_window = []\n",
    "    current_activity = None\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        if 'begin' in row['Begin_End']:\n",
    "            current_activity = row['Activity']\n",
    "            current_window = []\n",
    "        \n",
    "        if current_activity:\n",
    "            current_window.append(row)\n",
    "        \n",
    "        if 'end' in row['Begin_End']:\n",
    "            if current_window:\n",
    "                windows.append((current_activity, pd.DataFrame(current_window)))\n",
    "            current_activity = None\n",
    "            current_window = []\n",
    "\n",
    "    return windows\n",
    "\n",
    "activity_windows = create_activity_windows(aruba_data)\n",
    "print(f\"Number of activity windows: {len(activity_windows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Generating TDOST Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_type(sensor_id):\n",
    "    sensor_mapping = {\n",
    "        'M': 'Motion',\n",
    "        'D': 'Door',\n",
    "        'T': 'Temperature',\n",
    "        'L': 'Light',\n",
    "        'I': 'Item'\n",
    "    }\n",
    "    # Extract the first character of the sensor ID to determine the type\n",
    "    sensor_type_code = sensor_id[0]\n",
    "    # Return the corresponding sensor type, or 'Unknown' if not mapped\n",
    "    return sensor_mapping.get(sensor_type_code, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_location(sensor_id):\n",
    "    # Dictionary mapping sensor numbers to locations\n",
    "    location_mapping = {\n",
    "        '01': 'Living Room',\n",
    "        '02': 'Kitchen',\n",
    "        '03': 'Bedroom 1',\n",
    "        '04': 'Bedroom 2',\n",
    "        '05': 'Bathroom',\n",
    "        '06': 'Hallway',\n",
    "        '07': 'Garage',\n",
    "        '08': 'Front Door',\n",
    "        '09': 'Back Door',\n",
    "        '10': 'Dining Room',\n",
    "        '11': 'Office',\n",
    "        '12': 'Laundry Room',\n",
    "        '13': 'Basement',\n",
    "        '14': 'Stairs',\n",
    "        '15': 'Closet',\n",
    "        '16': 'Porch',\n",
    "        '17': 'Attic'\n",
    "    }\n",
    "    \n",
    "    # Extract the numeric part of the sensor ID\n",
    "    sensor_number = sensor_id[1:].zfill(2)  # Assumes first character is the type, e.g., M, D, T\n",
    "    \n",
    "    # Get the location based on the numeric part, or default to \"Other\"\n",
    "    return location_mapping.get(sensor_number, 'Other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timestamp_to_words(timestamp):\n",
    "    # Parse the timestamp\n",
    "\n",
    "    # Ensure the input is a datetime object\n",
    "    if isinstance(timestamp, pd.Timestamp):\n",
    "        dt = timestamp.to_pydatetime()\n",
    "    elif isinstance(timestamp, str):\n",
    "        # Attempt to parse with milliseconds first\n",
    "        try:\n",
    "            dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            # Fall back to parsing without milliseconds\n",
    "            dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported timestamp type\")\n",
    "    \n",
    "    # Extract hour, minute, and determine AM/PM\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    period = \"AM\" if hour < 12 else \"PM\"\n",
    "    \n",
    "    # Adjust hour for 12-hour format\n",
    "    hour = hour % 12 or 12  # 0 becomes 12 for AM/PM format\n",
    "\n",
    "    # Convert hour and minute to words\n",
    "    hour_text = num_to_words(hour)\n",
    "    minute_text = num_to_words(minute)\n",
    "\n",
    "    # Form the final text\n",
    "    return f\"{hour_text} hours {minute_text} minutes {period}\"\n",
    "\n",
    "def num_to_words(n):\n",
    "    # Dictionary to convert numbers to words for 0-59\n",
    "    words = {\n",
    "        0: \"zero\", 1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\", 5: \"five\",\n",
    "        6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\", 10: \"ten\",\n",
    "        11: \"eleven\", 12: \"twelve\", 13: \"thirteen\", 14: \"fourteen\",\n",
    "        15: \"fifteen\", 16: \"sixteen\", 17: \"seventeen\", 18: \"eighteen\",\n",
    "        19: \"nineteen\", 20: \"twenty\", 21: \"twenty-one\", 22: \"twenty-two\",\n",
    "        23: \"twenty-three\", 24: \"twenty-four\", 25: \"twenty-five\",\n",
    "        26: \"twenty-six\", 27: \"twenty-seven\", 28: \"twenty-eight\",\n",
    "        29: \"twenty-nine\", 30: \"thirty\", 31: \"thirty-one\", 32: \"thirty-two\",\n",
    "        33: \"thirty-three\", 34: \"thirty-four\", 35: \"thirty-five\",\n",
    "        36: \"thirty-six\", 37: \"thirty-seven\", 38: \"thirty-eight\",\n",
    "        39: \"thirty-nine\", 40: \"forty\", 41: \"forty-one\", 42: \"forty-two\",\n",
    "        43: \"forty-three\", 44: \"forty-four\", 45: \"forty-five\",\n",
    "        46: \"forty-six\", 47: \"forty-seven\", 48: \"forty-eight\",\n",
    "        49: \"forty-nine\", 50: \"fifty\", 51: \"fifty-one\", 52: \"fifty-two\",\n",
    "        53: \"fifty-three\", 54: \"fifty-four\", 55: \"fifty-five\",\n",
    "        56: \"fifty-six\", 57: \"fifty-seven\", 58: \"fifty-eight\",\n",
    "        59: \"fifty-nine\"\n",
    "    }\n",
    "    return words.get(n, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Generate TDOST Descriptions for Each Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Value</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Begin_End</th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>TDOST_Basic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M003</td>\n",
       "      <td>ON</td>\n",
       "      <td>Sleeping</td>\n",
       "      <td>begin</td>\n",
       "      <td>2010-11-04 00:03:50.209589</td>\n",
       "      <td>At approximately twelve hours three minutes AM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M003</td>\n",
       "      <td>OFF</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:03:57.399391</td>\n",
       "      <td>At approximately twelve hours three minutes AM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T002</td>\n",
       "      <td>21.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:15:08.984841</td>\n",
       "      <td>At approximately twelve hours fifteen minutes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T003</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:30:19.185547</td>\n",
       "      <td>At approximately twelve hours thirty minutes A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T004</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:30:19.385336</td>\n",
       "      <td>At approximately twelve hours thirty minutes A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sensor Value  Activity Begin_End                  Date_Time  \\\n",
       "0   M003    ON  Sleeping     begin 2010-11-04 00:03:50.209589   \n",
       "1   M003   OFF                     2010-11-04 00:03:57.399391   \n",
       "2   T002  21.5                     2010-11-04 00:15:08.984841   \n",
       "3   T003    21                     2010-11-04 00:30:19.185547   \n",
       "4   T004    21                     2010-11-04 00:30:19.385336   \n",
       "\n",
       "                                         TDOST_Basic  \n",
       "0  At approximately twelve hours three minutes AM...  \n",
       "1  At approximately twelve hours three minutes AM...  \n",
       "2  At approximately twelve hours fifteen minutes ...  \n",
       "3  At approximately twelve hours thirty minutes A...  \n",
       "4  At approximately twelve hours thirty minutes A...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate TDOST for each event\n",
    "def generate_tdost_basic(row):\n",
    "    curr_sensor_time = timestamp_to_words(row['Date_Time'])\n",
    "    # timestamp = row['Date_Time'].strftime('%H:%M:%S')\n",
    "    curr_sensor_type = get_sensor_type(row['Sensor'])\n",
    "    curr_sensor_location = get_sensor_location(row['Sensor'])\n",
    "\n",
    "    curr_sensor_value = row['Sensor']\n",
    "    if curr_sensor_value != 'ON' or curr_sensor_value != 'OFF':\n",
    "        curr_sensor_value = num_to_words(curr_sensor_value)\n",
    "\n",
    "    description = f\"At approximately {curr_sensor_time}, {curr_sensor_type} sensor in {curr_sensor_location} fired with value {curr_sensor_value}.\"\n",
    "    return description\n",
    "\n",
    "def process_windows(windows):\n",
    "    processed = []\n",
    "    for activity, window in windows:\n",
    "        window['TDOST_Basic'] = window.apply(generate_tdost_basic, axis=1)\n",
    "        processed.append((activity, window))\n",
    "    return processed\n",
    "\n",
    "processed_windows = process_windows(activity_windows)\n",
    "processed_windows[0][1].head()\n",
    "# Show a sample processed window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    At approximately twelve hours three minutes AM...\n",
      "1    At approximately twelve hours three minutes AM...\n",
      "2    At approximately twelve hours fifteen minutes ...\n",
      "3    At approximately twelve hours thirty minutes A...\n",
      "4    At approximately twelve hours thirty minutes A...\n",
      "Name: TDOST_Basic, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_windows[0][1]['TDOST_Basic'].iloc[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Encode TDOST Descriptions and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6441\n",
      "2 / 6441\n",
      "3 / 6441\n",
      "4 / 6441\n",
      "5 / 6441\n",
      "6 / 6441\n",
      "7 / 6441\n",
      "8 / 6441\n",
      "9 / 6441\n",
      "10 / 6441\n",
      "11 / 6441\n",
      "12 / 6441\n",
      "13 / 6441\n",
      "14 / 6441\n",
      "15 / 6441\n",
      "16 / 6441\n",
      "17 / 6441\n",
      "18 / 6441\n",
      "19 / 6441\n",
      "20 / 6441\n",
      "21 / 6441\n",
      "22 / 6441\n",
      "23 / 6441\n",
      "24 / 6441\n",
      "25 / 6441\n",
      "26 / 6441\n",
      "27 / 6441\n",
      "28 / 6441\n",
      "29 / 6441\n",
      "30 / 6441\n",
      "31 / 6441\n",
      "32 / 6441\n",
      "33 / 6441\n",
      "34 / 6441\n",
      "35 / 6441\n",
      "36 / 6441\n",
      "37 / 6441\n",
      "38 / 6441\n",
      "39 / 6441\n",
      "40 / 6441\n",
      "41 / 6441\n",
      "42 / 6441\n",
      "43 / 6441\n",
      "44 / 6441\n",
      "45 / 6441\n",
      "46 / 6441\n",
      "47 / 6441\n",
      "48 / 6441\n",
      "49 / 6441\n",
      "50 / 6441\n",
      "51 / 6441\n",
      "52 / 6441\n",
      "53 / 6441\n",
      "54 / 6441\n",
      "55 / 6441\n",
      "56 / 6441\n",
      "57 / 6441\n",
      "58 / 6441\n",
      "59 / 6441\n",
      "60 / 6441\n",
      "61 / 6441\n",
      "62 / 6441\n",
      "63 / 6441\n",
      "64 / 6441\n",
      "65 / 6441\n",
      "66 / 6441\n",
      "67 / 6441\n",
      "68 / 6441\n",
      "69 / 6441\n",
      "70 / 6441\n",
      "71 / 6441\n",
      "72 / 6441\n",
      "73 / 6441\n",
      "74 / 6441\n",
      "75 / 6441\n",
      "76 / 6441\n",
      "77 / 6441\n",
      "78 / 6441\n",
      "79 / 6441\n",
      "80 / 6441\n",
      "81 / 6441\n",
      "82 / 6441\n",
      "83 / 6441\n",
      "84 / 6441\n",
      "85 / 6441\n",
      "86 / 6441\n",
      "87 / 6441\n",
      "88 / 6441\n",
      "89 / 6441\n",
      "90 / 6441\n",
      "91 / 6441\n",
      "92 / 6441\n",
      "93 / 6441\n",
      "94 / 6441\n",
      "95 / 6441\n",
      "96 / 6441\n",
      "97 / 6441\n",
      "98 / 6441\n",
      "99 / 6441\n",
      "100 / 6441\n",
      "101 / 6441\n",
      "102 / 6441\n",
      "103 / 6441\n",
      "104 / 6441\n",
      "105 / 6441\n",
      "106 / 6441\n",
      "107 / 6441\n",
      "108 / 6441\n",
      "109 / 6441\n",
      "110 / 6441\n",
      "111 / 6441\n",
      "112 / 6441\n",
      "113 / 6441\n",
      "114 / 6441\n",
      "115 / 6441\n",
      "116 / 6441\n",
      "117 / 6441\n",
      "118 / 6441\n",
      "119 / 6441\n",
      "120 / 6441\n",
      "121 / 6441\n",
      "122 / 6441\n",
      "123 / 6441\n",
      "124 / 6441\n",
      "125 / 6441\n",
      "126 / 6441\n",
      "127 / 6441\n",
      "128 / 6441\n",
      "129 / 6441\n",
      "130 / 6441\n",
      "131 / 6441\n",
      "132 / 6441\n",
      "133 / 6441\n",
      "134 / 6441\n",
      "135 / 6441\n",
      "136 / 6441\n",
      "137 / 6441\n",
      "138 / 6441\n",
      "139 / 6441\n",
      "140 / 6441\n",
      "141 / 6441\n",
      "142 / 6441\n",
      "143 / 6441\n",
      "144 / 6441\n",
      "145 / 6441\n",
      "146 / 6441\n",
      "147 / 6441\n",
      "148 / 6441\n",
      "149 / 6441\n",
      "150 / 6441\n",
      "151 / 6441\n",
      "152 / 6441\n",
      "153 / 6441\n",
      "154 / 6441\n",
      "155 / 6441\n",
      "156 / 6441\n",
      "157 / 6441\n",
      "158 / 6441\n",
      "159 / 6441\n",
      "160 / 6441\n",
      "161 / 6441\n",
      "162 / 6441\n",
      "163 / 6441\n",
      "164 / 6441\n",
      "165 / 6441\n",
      "166 / 6441\n",
      "167 / 6441\n",
      "168 / 6441\n",
      "169 / 6441\n",
      "170 / 6441\n",
      "171 / 6441\n",
      "172 / 6441\n",
      "173 / 6441\n",
      "174 / 6441\n",
      "175 / 6441\n",
      "176 / 6441\n",
      "177 / 6441\n",
      "178 / 6441\n",
      "179 / 6441\n",
      "180 / 6441\n",
      "181 / 6441\n",
      "182 / 6441\n",
      "183 / 6441\n",
      "184 / 6441\n",
      "185 / 6441\n",
      "186 / 6441\n",
      "187 / 6441\n",
      "188 / 6441\n",
      "189 / 6441\n",
      "190 / 6441\n",
      "191 / 6441\n",
      "192 / 6441\n",
      "193 / 6441\n",
      "194 / 6441\n",
      "195 / 6441\n",
      "196 / 6441\n",
      "197 / 6441\n",
      "198 / 6441\n",
      "199 / 6441\n",
      "200 / 6441\n",
      "201 / 6441\n",
      "202 / 6441\n",
      "203 / 6441\n",
      "204 / 6441\n",
      "205 / 6441\n",
      "206 / 6441\n",
      "207 / 6441\n",
      "208 / 6441\n",
      "209 / 6441\n",
      "210 / 6441\n",
      "211 / 6441\n",
      "212 / 6441\n",
      "213 / 6441\n",
      "214 / 6441\n",
      "215 / 6441\n",
      "216 / 6441\n",
      "217 / 6441\n",
      "218 / 6441\n",
      "219 / 6441\n",
      "220 / 6441\n",
      "221 / 6441\n",
      "222 / 6441\n",
      "223 / 6441\n",
      "224 / 6441\n",
      "225 / 6441\n",
      "226 / 6441\n",
      "227 / 6441\n",
      "228 / 6441\n",
      "229 / 6441\n",
      "230 / 6441\n",
      "231 / 6441\n",
      "232 / 6441\n",
      "233 / 6441\n",
      "234 / 6441\n",
      "235 / 6441\n",
      "236 / 6441\n",
      "237 / 6441\n",
      "238 / 6441\n",
      "239 / 6441\n",
      "240 / 6441\n",
      "241 / 6441\n",
      "242 / 6441\n",
      "243 / 6441\n",
      "244 / 6441\n",
      "245 / 6441\n",
      "246 / 6441\n",
      "247 / 6441\n",
      "248 / 6441\n",
      "249 / 6441\n",
      "250 / 6441\n",
      "251 / 6441\n",
      "252 / 6441\n",
      "253 / 6441\n",
      "254 / 6441\n",
      "255 / 6441\n",
      "256 / 6441\n",
      "257 / 6441\n",
      "258 / 6441\n",
      "259 / 6441\n",
      "260 / 6441\n",
      "261 / 6441\n",
      "262 / 6441\n",
      "263 / 6441\n",
      "264 / 6441\n",
      "265 / 6441\n",
      "266 / 6441\n",
      "267 / 6441\n",
      "268 / 6441\n",
      "269 / 6441\n",
      "270 / 6441\n",
      "271 / 6441\n",
      "272 / 6441\n",
      "273 / 6441\n",
      "274 / 6441\n",
      "275 / 6441\n",
      "276 / 6441\n",
      "277 / 6441\n",
      "278 / 6441\n",
      "279 / 6441\n",
      "280 / 6441\n",
      "281 / 6441\n",
      "282 / 6441\n",
      "283 / 6441\n",
      "284 / 6441\n",
      "285 / 6441\n",
      "286 / 6441\n",
      "287 / 6441\n",
      "288 / 6441\n",
      "289 / 6441\n",
      "290 / 6441\n",
      "291 / 6441\n",
      "292 / 6441\n",
      "293 / 6441\n",
      "294 / 6441\n",
      "295 / 6441\n",
      "296 / 6441\n",
      "297 / 6441\n",
      "298 / 6441\n",
      "299 / 6441\n",
      "300 / 6441\n",
      "301 / 6441\n",
      "302 / 6441\n",
      "303 / 6441\n",
      "304 / 6441\n",
      "305 / 6441\n",
      "306 / 6441\n",
      "307 / 6441\n",
      "308 / 6441\n",
      "309 / 6441\n",
      "310 / 6441\n",
      "311 / 6441\n",
      "312 / 6441\n",
      "313 / 6441\n",
      "314 / 6441\n",
      "315 / 6441\n",
      "316 / 6441\n",
      "317 / 6441\n",
      "318 / 6441\n",
      "319 / 6441\n",
      "320 / 6441\n",
      "321 / 6441\n",
      "322 / 6441\n",
      "323 / 6441\n",
      "324 / 6441\n",
      "325 / 6441\n",
      "326 / 6441\n",
      "327 / 6441\n",
      "328 / 6441\n",
      "329 / 6441\n",
      "330 / 6441\n",
      "331 / 6441\n",
      "332 / 6441\n",
      "333 / 6441\n",
      "334 / 6441\n",
      "335 / 6441\n",
      "336 / 6441\n",
      "337 / 6441\n",
      "338 / 6441\n",
      "339 / 6441\n",
      "340 / 6441\n",
      "341 / 6441\n",
      "342 / 6441\n",
      "343 / 6441\n",
      "344 / 6441\n",
      "345 / 6441\n",
      "346 / 6441\n",
      "347 / 6441\n",
      "348 / 6441\n",
      "349 / 6441\n",
      "350 / 6441\n",
      "351 / 6441\n",
      "352 / 6441\n",
      "353 / 6441\n",
      "354 / 6441\n",
      "355 / 6441\n",
      "356 / 6441\n",
      "357 / 6441\n",
      "358 / 6441\n",
      "359 / 6441\n",
      "360 / 6441\n",
      "361 / 6441\n",
      "362 / 6441\n",
      "363 / 6441\n",
      "364 / 6441\n",
      "365 / 6441\n",
      "366 / 6441\n",
      "367 / 6441\n",
      "368 / 6441\n",
      "369 / 6441\n",
      "370 / 6441\n",
      "371 / 6441\n",
      "372 / 6441\n",
      "373 / 6441\n",
      "374 / 6441\n",
      "375 / 6441\n",
      "376 / 6441\n",
      "377 / 6441\n",
      "378 / 6441\n",
      "379 / 6441\n",
      "380 / 6441\n",
      "381 / 6441\n",
      "382 / 6441\n",
      "383 / 6441\n",
      "384 / 6441\n",
      "385 / 6441\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m activity, window \u001b[38;5;129;01min\u001b[39;00m processed_windows:\n\u001b[0;32m---> 11\u001b[0m     tdost_embeddings \u001b[38;5;241m=\u001b[39m sentence_model\u001b[38;5;241m.\u001b[39mencode(window[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTDOST\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     12\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(tdost_embeddings)\n\u001b[1;32m     13\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(activity)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:621\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 621\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    623\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:688\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    687\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:350\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    348\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    351\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:942\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    935\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[1;32m    936\u001b[0m             attention_mask,\n\u001b[1;32m    937\u001b[0m             input_shape,\n\u001b[1;32m    938\u001b[0m             embedding_output,\n\u001b[1;32m    939\u001b[0m             past_key_values_length,\n\u001b[1;32m    940\u001b[0m         )\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[1;32m    943\u001b[0m             attention_mask, embedding_output\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39mseq_length\n\u001b[1;32m    944\u001b[0m         )\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:447\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AttentionMaskConverter\u001b[38;5;241m.\u001b[39m_expand_mask(mask\u001b[38;5;241m=\u001b[39mmask, dtype\u001b[38;5;241m=\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39mtgt_len)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:186\u001b[0m, in \u001b[0;36mAttentionMaskConverter._expand_mask\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    182\u001b[0m tgt_len \u001b[38;5;241m=\u001b[39m tgt_len \u001b[38;5;28;01mif\u001b[39;00m tgt_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m src_len\n\u001b[1;32m    184\u001b[0m expanded_mask \u001b[38;5;241m=\u001b[39m mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\u001b[38;5;241m.\u001b[39mexpand(bsz, \u001b[38;5;241m1\u001b[39m, tgt_len, src_len)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[0;32m--> 186\u001b[0m inverted_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m expanded_mask\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inverted_mask\u001b[38;5;241m.\u001b[39mmasked_fill(inverted_mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool), torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:39\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:1028\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m-> 1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _C\u001b[38;5;241m.\u001b[39m_VariableFunctions\u001b[38;5;241m.\u001b[39mrsub(\u001b[38;5;28mself\u001b[39m, other)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load pre-trained Sentence Transformer\n",
    "sentence_model = SentenceTransformer('all-distilroberta-v1')\n",
    "\n",
    "# Encode descriptions and labels\n",
    "X, y = [], []\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "windows_len = len(processed_windows)\n",
    "count = 0\n",
    "for activity, window in processed_windows:\n",
    "    tdost_embeddings = sentence_model.encode(window['TDOST'].tolist())\n",
    "    X.append(tdost_embeddings)\n",
    "    y.append(activity)\n",
    "    count = count + 1\n",
    "    print(f\"{count} / {windows_len}\")\n",
    "\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Encoded {len(X)} windows with {num_classes} activity classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Pad Sequences and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences to make them uniform length\n",
    "max_length = max(len(seq) for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_length, dtype='float32', padding='post')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Build and Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(max_length, X_padded.shape[2])),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train_cat, validation_data=(X_test, y_test_cat), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Test on new window (replace with real test window)\n",
    "sample_window = X_test[0].reshape(1, max_length, -1)\n",
    "prediction = model.predict(sample_window)\n",
    "predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "print(f\"Predicted Activity: {predicted_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
