{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.17,>=2.16 (from tf-keras)\n",
      "  Downloading tensorflow-2.16.2-cp312-cp312-macosx_10_15_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading grpcio-1.68.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading optree-0.13.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.16.2-cp312-cp312-macosx_10_15_x86_64.whl (259.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.0-cp312-cp312-macosx_10_9_universal2.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl (393 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-macosx_10_13_universal2.whl (600 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.0/601.0 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow, tf-keras\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.0 keras-3.6.0 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 termcolor-2.5.0 tf-keras-2.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/525pmdvn1xd4497v9mhtzkl40000gn/T/ipykernel_7179/3024261080.py:10: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  aruba_data = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Value</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Begin_End</th>\n",
       "      <th>Date_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M003</td>\n",
       "      <td>ON</td>\n",
       "      <td>Sleeping</td>\n",
       "      <td>begin</td>\n",
       "      <td>2010-11-04 00:03:50.209589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M003</td>\n",
       "      <td>OFF</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:03:57.399391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T002</td>\n",
       "      <td>21.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:15:08.984841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T003</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:30:19.185547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T004</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:30:19.385336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sensor Value  Activity Begin_End                  Date_Time\n",
       "0   M003    ON  Sleeping     begin 2010-11-04 00:03:50.209589\n",
       "1   M003   OFF                     2010-11-04 00:03:57.399391\n",
       "2   T002  21.5                     2010-11-04 00:15:08.984841\n",
       "3   T003    21                     2010-11-04 00:30:19.185547\n",
       "4   T004    21                     2010-11-04 00:30:19.385336"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMAC\n",
    "aruba_data_path = '/Users/harrisonkirstein/Documents/GitHub/CSCI-4380-Honors-Option-Repo/CSCI 4380 Honors Option Project/Datasets/aruba/data'\n",
    "\n",
    "\n",
    "# LAPTOP\n",
    "# aruba_data_path = '/Users/harrisonkirstein/Desktop/CSCI-4380-Honors-Option-Repo/CSCI 4380 Honors Option Project/Datasets/aruba/data'\n",
    "\n",
    "\n",
    "# Load dataset with variable columns\n",
    "aruba_data = pd.read_csv(\n",
    "    aruba_data_path, \n",
    "    header=None, \n",
    "    names=['Date', 'Time', 'Sensor', 'Value', 'Activity', 'Begin_End'], \n",
    "    delim_whitespace=True,\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "# Combine Date and Time into a single timestamp column\n",
    "aruba_data['Date_Time'] = pd.to_datetime(aruba_data['Date'] + ' ' + aruba_data['Time'], errors='coerce')\n",
    "aruba_data.drop(columns=['Date', 'Time'], inplace=True)\n",
    "\n",
    "# Fill missing columns with NaN for rows without activity labels\n",
    "aruba_data.fillna('', inplace=True)\n",
    "\n",
    "# Preview the dataset\n",
    "aruba_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Value</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Begin_End</th>\n",
       "      <th>Date_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1476693</th>\n",
       "      <td>c</td>\n",
       "      <td>OFF</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2011-05-10 18:42:45.169231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sensor Value Activity Begin_End                  Date_Time\n",
       "1476693      c   OFF                    2011-05-10 18:42:45.169231"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aruba_data[aruba_data['Sensor'] == 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create Activity Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of activity windows: 6441\n"
     ]
    }
   ],
   "source": [
    "# Group sensor data into activity windows\n",
    "def create_activity_windows(data):\n",
    "    windows = []\n",
    "    current_window = []\n",
    "    current_activity = None\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        if 'begin' in row['Begin_End']:\n",
    "            current_activity = row['Activity']\n",
    "            current_window = []\n",
    "        \n",
    "        if current_activity:\n",
    "            current_window.append(row)\n",
    "        \n",
    "        if 'end' in row['Begin_End']:\n",
    "            if current_window:\n",
    "                windows.append((current_activity, pd.DataFrame(current_window)))\n",
    "            current_activity = None\n",
    "            current_window = []\n",
    "\n",
    "    return windows\n",
    "\n",
    "activity_windows = create_activity_windows(aruba_data)\n",
    "print(f\"Number of activity windows: {len(activity_windows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Generating TDOST Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_type(sensor_id):\n",
    "    sensor_mapping = {\n",
    "        'M': 'Motion',\n",
    "        'D': 'Door',\n",
    "        'T': 'Temperature',\n",
    "        'L': 'Light',\n",
    "        'I': 'Item'\n",
    "    }\n",
    "    # Extract the first character of the sensor ID to determine the type\n",
    "    sensor_type_code = sensor_id[0]\n",
    "    # Return the corresponding sensor type, or 'Unknown' if not mapped\n",
    "    return sensor_mapping.get(sensor_type_code, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_location(sensor_id):\n",
    "    # Dictionary mapping sensor numbers to locations\n",
    "    location_mapping = {\n",
    "        'M001': 'Bedroom Near Closet',\n",
    "        'M002': 'Bedroom',\n",
    "        'M003': 'Bedroom',\n",
    "        'M004': 'Bedroom near bathroom',\n",
    "        'M005': 'Bedroom Hallway',\n",
    "        'M006': 'Bedroom Door',\n",
    "        'M007': 'Bedroom',\n",
    "        'M008': 'Hallway',\n",
    "        'M009': 'Living Room Couch',\n",
    "        'M010': 'Living Room Couch',\n",
    "        'M011': 'Front Door',\n",
    "        'M012': 'Living Room',\n",
    "        'M013': 'Living Room',\n",
    "        'M014': 'Dining Room',\n",
    "        'M015': 'Kitchen Stove',\n",
    "        'M016': 'Back Door',\n",
    "        'M017': 'Back Door',\n",
    "        'M018': 'Kitchen Entry',\n",
    "        'M019': 'Kitchen',\n",
    "        'M021': 'Hallway 2',\n",
    "        'M022': 'Hallway 3',\n",
    "        'M023': 'Bedroom 2 Door',\n",
    "        'M024': 'Bedroom 2',\n",
    "        'M025': 'Office Tabke',\n",
    "        'M026': 'Office Desk',\n",
    "        'M027': 'Office',\n",
    "        'M029': 'Bathroom Door',\n",
    "        'M030': 'Garage Door',\n",
    "        'M031': 'Housekeeping Closet',\n",
    "        'D001': 'Front Door',\n",
    "        'D002': 'Back Door',\n",
    "        'D003': 'Closet Door',\n",
    "        'D004': 'Garage Door',\n",
    "        'T001': 'Bedroom',\n",
    "        'T002': 'Living Room',\n",
    "        'T003': 'Kitchen',\n",
    "        'T004': 'Hallway'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Get the location based on the numeric part, or default to \"Other\"\n",
    "    return location_mapping.get(sensor_id, 'Other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timestamp_to_words(timestamp):\n",
    "    # Parse the timestamp\n",
    "\n",
    "    # Ensure the input is a datetime object\n",
    "    if isinstance(timestamp, pd.Timestamp):\n",
    "        dt = timestamp.to_pydatetime()\n",
    "    elif isinstance(timestamp, str):\n",
    "        # Attempt to parse with milliseconds first\n",
    "        try:\n",
    "            dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            # Fall back to parsing without milliseconds\n",
    "            dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported timestamp type\")\n",
    "    \n",
    "    # Extract hour, minute, and determine AM/PM\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    period = \"AM\" if hour < 12 else \"PM\"\n",
    "    \n",
    "    # Adjust hour for 12-hour format\n",
    "    hour = hour % 12 or 12  # 0 becomes 12 for AM/PM format\n",
    "\n",
    "    # Convert hour and minute to words\n",
    "    hour_text = num_to_words(hour)\n",
    "    minute_text = num_to_words(minute)\n",
    "\n",
    "    # Form the final text\n",
    "    return f\"{hour_text} hours {minute_text} minutes {period}\"\n",
    "\n",
    "def num_to_words(n):\n",
    "    # Dictionary to convert numbers to words for 0-59\n",
    "    words = {\n",
    "        0: \"zero\", 1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\", 5: \"five\",\n",
    "        6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\", 10: \"ten\",\n",
    "        11: \"eleven\", 12: \"twelve\", 13: \"thirteen\", 14: \"fourteen\",\n",
    "        15: \"fifteen\", 16: \"sixteen\", 17: \"seventeen\", 18: \"eighteen\",\n",
    "        19: \"nineteen\", 20: \"twenty\", 21: \"twenty-one\", 22: \"twenty-two\",\n",
    "        23: \"twenty-three\", 24: \"twenty-four\", 25: \"twenty-five\",\n",
    "        26: \"twenty-six\", 27: \"twenty-seven\", 28: \"twenty-eight\",\n",
    "        29: \"twenty-nine\", 30: \"thirty\", 31: \"thirty-one\", 32: \"thirty-two\",\n",
    "        33: \"thirty-three\", 34: \"thirty-four\", 35: \"thirty-five\",\n",
    "        36: \"thirty-six\", 37: \"thirty-seven\", 38: \"thirty-eight\",\n",
    "        39: \"thirty-nine\", 40: \"forty\", 41: \"forty-one\", 42: \"forty-two\",\n",
    "        43: \"forty-three\", 44: \"forty-four\", 45: \"forty-five\",\n",
    "        46: \"forty-six\", 47: \"forty-seven\", 48: \"forty-eight\",\n",
    "        49: \"forty-nine\", 50: \"fifty\", 51: \"fifty-one\", 52: \"fifty-two\",\n",
    "        53: \"fifty-three\", 54: \"fifty-four\", 55: \"fifty-five\",\n",
    "        56: \"fifty-six\", 57: \"fifty-seven\", 58: \"fifty-eight\",\n",
    "        59: \"fifty-nine\"\n",
    "    }\n",
    "    return words.get(n, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Generate TDOST Descriptions for Each Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Value</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Begin_End</th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>TDOST_Basic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M003</td>\n",
       "      <td>ON</td>\n",
       "      <td>Sleeping</td>\n",
       "      <td>begin</td>\n",
       "      <td>2010-11-04 00:03:50.209589</td>\n",
       "      <td>At approximately twelve hours three minutes AM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M003</td>\n",
       "      <td>OFF</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:03:57.399391</td>\n",
       "      <td>At approximately twelve hours three minutes AM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T002</td>\n",
       "      <td>21.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:15:08.984841</td>\n",
       "      <td>At approximately twelve hours fifteen minutes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T003</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:30:19.185547</td>\n",
       "      <td>At approximately twelve hours thirty minutes A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T004</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-11-04 00:30:19.385336</td>\n",
       "      <td>At approximately twelve hours thirty minutes A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sensor Value  Activity Begin_End                  Date_Time  \\\n",
       "0   M003    ON  Sleeping     begin 2010-11-04 00:03:50.209589   \n",
       "1   M003   OFF                     2010-11-04 00:03:57.399391   \n",
       "2   T002  21.5                     2010-11-04 00:15:08.984841   \n",
       "3   T003    21                     2010-11-04 00:30:19.185547   \n",
       "4   T004    21                     2010-11-04 00:30:19.385336   \n",
       "\n",
       "                                         TDOST_Basic  \n",
       "0  At approximately twelve hours three minutes AM...  \n",
       "1  At approximately twelve hours three minutes AM...  \n",
       "2  At approximately twelve hours fifteen minutes ...  \n",
       "3  At approximately twelve hours thirty minutes A...  \n",
       "4  At approximately twelve hours thirty minutes A...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate TDOST for each event\n",
    "def generate_tdost_basic(row):\n",
    "    curr_sensor_time = timestamp_to_words(row['Date_Time'])\n",
    "    # timestamp = row['Date_Time'].strftime('%H:%M:%S')\n",
    "    curr_sensor_type = get_sensor_type(row['Sensor'])\n",
    "    curr_sensor_location = get_sensor_location(row['Sensor'])\n",
    "\n",
    "    curr_sensor_value = row['Sensor']\n",
    "    if curr_sensor_value != 'ON' or curr_sensor_value != 'OFF':\n",
    "        curr_sensor_value = num_to_words(curr_sensor_value)\n",
    "\n",
    "    description = f\"At approximately {curr_sensor_time}, {curr_sensor_type} sensor in {curr_sensor_location} fired with value {curr_sensor_value}.\"\n",
    "    return description\n",
    "\n",
    "def process_windows(windows):\n",
    "    processed = []\n",
    "    for activity, window in windows:\n",
    "        window['TDOST_Basic'] = window.apply(generate_tdost_basic, axis=1)\n",
    "        processed.append((activity, window))\n",
    "    return processed\n",
    "\n",
    "processed_windows = process_windows(activity_windows)\n",
    "processed_windows[0][1].head()\n",
    "# Show a sample processed window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    At approximately twelve hours three minutes AM...\n",
       "1    At approximately twelve hours three minutes AM...\n",
       "2    At approximately twelve hours fifteen minutes ...\n",
       "3    At approximately twelve hours thirty minutes A...\n",
       "4    At approximately twelve hours thirty minutes A...\n",
       "Name: TDOST_Basic, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_windows[0][1]['TDOST_Basic'].iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Encode TDOST Descriptions and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6441\n",
      "2 / 6441\n",
      "3 / 6441\n",
      "4 / 6441\n",
      "5 / 6441\n",
      "6 / 6441\n",
      "7 / 6441\n",
      "8 / 6441\n",
      "9 / 6441\n",
      "10 / 6441\n",
      "11 / 6441\n",
      "12 / 6441\n",
      "13 / 6441\n",
      "14 / 6441\n",
      "15 / 6441\n",
      "16 / 6441\n",
      "17 / 6441\n",
      "18 / 6441\n",
      "19 / 6441\n",
      "20 / 6441\n",
      "21 / 6441\n",
      "22 / 6441\n",
      "23 / 6441\n",
      "24 / 6441\n",
      "25 / 6441\n",
      "26 / 6441\n",
      "27 / 6441\n",
      "28 / 6441\n",
      "29 / 6441\n",
      "30 / 6441\n",
      "31 / 6441\n",
      "32 / 6441\n",
      "33 / 6441\n",
      "34 / 6441\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Sentence Transformer\n",
    "sentence_model = SentenceTransformer('all-distilroberta-v1')\n",
    "\n",
    "# Encode descriptions and labels\n",
    "X, y = [], []\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "windows_len = len(processed_windows)\n",
    "count = 0\n",
    "for activity, window in processed_windows:\n",
    "    tdost_embeddings = sentence_model.encode(window['TDOST_Basic'].tolist())\n",
    "    X.append(tdost_embeddings)\n",
    "    y.append(activity)\n",
    "    count = count + 1\n",
    "    print(f\"{count} / {windows_len}\")\n",
    "\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Encoded {len(X)} windows with {num_classes} activity classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Pad Sequences and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences to make them uniform length\n",
    "max_length = max(len(seq) for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_length, dtype='float32', padding='post')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Build and Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(max_length, X_padded.shape[2])),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train_cat, validation_data=(X_test, y_test_cat), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Test on new window (replace with real test window)\n",
    "sample_window = X_test[0].reshape(1, max_length, -1)\n",
    "prediction = model.predict(sample_window)\n",
    "predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "print(f\"Predicted Activity: {predicted_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
